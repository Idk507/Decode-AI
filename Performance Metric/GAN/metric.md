
---

### 1. Inception Score (IS)

**Formula:**
<img width="998" height="290" alt="image" src="https://github.com/user-attachments/assets/ab6777a8-77ed-44fa-9e4e-26201bff11cc" />


**Explanation:**
The Inception Score measures the quality and diversity of generated images by evaluating two factors: (1) how confident a pre-trained classifier (e.g., Inception V3) is in assigning labels to generated images (quality), and (2) how diverse the predicted labels are across the generated set (diversity). A higher IS indicates better quality and diversity.

**When, Where, and Why It Is Used:**
- **When**: Used to evaluate the quality and diversity of images generated by GANs.
- **Where**: Common in image generation tasks, especially on datasets like ImageNet or CIFAR-10.
- **Why**: IS is a simple, automated metric that correlates somewhat with human perception of image quality and diversity.
- **Role**: Early standard metric for GAN evaluation, though less used today due to limitations.

**Advantages:**
- Easy to compute using a pre-trained classifier.
- Captures both quality (confident predictions) and diversity (varied labels).

**Limitations:**
- Dependent on the classifier (e.g., Inception V3), which may not generalize to all domains.
- Insensitive to mode collapse (if all generated images are high-quality but similar).
- Does not compare directly to real data distribution.
- Biased toward datasets similar to ImageNet (Inception’s training data).

**Example Use Case:**
In evaluating a GAN trained on CIFAR-10, IS measures how well the generated images are classified as distinct classes (e.g., dogs, cars) with high confidence and diversity.

---

### 2. Fréchet Inception Distance (FID)

**Formula:**
<img width="977" height="231" alt="image" src="https://github.com/user-attachments/assets/60cbee2e-746a-406e-9c19-62060384dc3f" />


**Explanation:**
FID measures the similarity between the feature distributions of real and generated images, using features from a pre-trained Inception V3 model. Lower FID indicates that generated images are more similar to real images in terms of quality and diversity.

**When, Where, and Why It Is Used:**
- **When**: Used to evaluate the quality and diversity of generated images in GANs.
- **Where**: Standard in image generation tasks (e.g., ImageNet, CelebA, LSUN).
- **Why**: FID compares distributions directly, capturing both visual quality and diversity, and is more robust than IS.
- **Role**: Widely adopted as a primary metric for GAN evaluation due to its reliability.

**Advantages:**
- Sensitive to both quality and diversity, including mode collapse.
- Correlates well with human perception.
- Compares directly to real data distribution.

**Limitations:**
- Dependent on Inception V3, which may not generalize to non-ImageNet-like data.
- Computationally expensive due to feature extraction.
- Assumes Gaussian feature distributions, which may not always hold.

**Example Use Case:**
In evaluating a GAN for face generation (e.g., CelebA), FID compares the distribution of generated faces to real faces, assessing realism and variety.

---

### 3. Precision/Recall (Distribution-based)

**Formula:**
- **Precision**: Fraction of generated samples that fall within the support of the real data distribution.
<img width="335" height="60" alt="image" src="https://github.com/user-attachments/assets/32d4b292-2ba8-4a76-9bc1-2ada597c71c7" />

- **Recall**: Fraction of real samples that fall within the support of the generated data distribution.
<img width="286" height="88" alt="image" src="https://github.com/user-attachments/assets/e45ee72a-2be1-4bcc-b2af-3828573af35b" />

- $\( x_g \)$: Generated sample.
- $\( x_r \)$ : Real sample.
- $\( S_r, S_g \)$: Support of real and generated distributions, often approximated using nearest neighbors in feature space (e.g., Inception V3 features).
- $\( X_r, X_g \)$ : Sets of real and generated samples.

**Explanation:**
Precision measures how many generated samples are realistic (i.e., lie within the real data distribution), while Recall measures how well the generated distribution covers the real data distribution (i.e., captures all modes). These are often computed using feature embeddings and nearest-neighbor methods.

**When, Where, and Why It Is Used:**
- **When**: Used to evaluate the trade-off between quality (precision) and diversity (recall) in GANs.
- **Where**: Common in image generation and other generative modeling tasks.
- **Why**: Separates quality and diversity, providing insights into mode collapse or over-optimization for realism.
- **Role**: Complements FID by breaking down performance into precision and recall components.

**Advantages:**
- Explicitly separates quality (precision) and diversity (recall).
- Detects mode collapse (low recall) and poor quality (low precision).
- More interpretable than FID for specific failure modes.

**Limitations:**
- Requires defining the support of distributions, often approximated heuristically.
- Computationally expensive due to nearest-neighbor searches.
- Dependent on feature space (e.g., Inception V3).

**Example Use Case:**
In a GAN generating artwork, Precision/Recall evaluates whether the generated images are realistic (high precision) and cover diverse styles (high recall).

---

### 4. Kernel Inception Distance (KID)

**Formula:**
<img width="1008" height="299" alt="image" src="https://github.com/user-attachments/assets/05921884-995b-4fcb-a055-72310a8a6e82" />


**Explanation:**
KID measures the difference between real and generated image distributions using Maximum Mean Discrepancy with a kernel function, applied to features from a pre-trained model. Lower KID indicates better similarity.

**When, Where, and Why It Is Used:**
- **When**: Used to evaluate GANs, particularly when FID’s Gaussian assumption is limiting.
- **Where**: Common in image generation tasks, especially for smaller datasets.
- **Why**: KID is less biased than FID for small sample sizes and does not assume Gaussian distributions.
- **Role**: Alternative to FID, especially for datasets where FID may be unreliable.

**Advantages:**
- Less biased for small sample sizes compared to FID.
- Does not assume Gaussian distributions.
- Captures both quality and diversity.

**Limitations:**
- Computationally expensive due to kernel calculations.
- Dependent on the choice of kernel and feature extractor.
- Less widely adopted than FID.

**Example Use Case:**
In evaluating a GAN on a small medical imaging dataset, KID provides a robust measure of distribution similarity without FID’s Gaussian assumption.

---

### 5. Wasserstein Distance

**Formula:**
<img width="995" height="260" alt="image" src="https://github.com/user-attachments/assets/ece3b49e-4c8d-4917-9f75-9991dd011885" />


**Explanation:**
Wasserstein Distance (also called Earth Mover’s Distance) measures the cost of transforming the generated distribution into the real distribution, often approximated in feature space or via GAN training losses.

**When, Where, and Why It Is Used:**
- **When**: Used as a loss function in Wasserstein GANs or as an evaluation metric for distribution similarity.
- **Where**: Common in GAN training and evaluation, particularly for image generation.
- **Why**: Provides a meaningful distance metric for distributions, capturing both quality and diversity, and is theoretically robust.
- **Role**: Used both as a training objective and an evaluation metric for GANs.

**Advantages:**
- Theoretically grounded, capturing distribution differences.
- Robust to mode collapse when used in training.
- Applicable to various data types.

**Limitations:**
- Computationally expensive to compute exactly; approximations are often used.
- Requires careful implementation in GAN training (e.g., Lipschitz constraints).
- Less intuitive as an evaluation metric compared to FID.

**Example Use Case:**
In Wasserstein GAN training, the Wasserstein Distance guides optimization to align generated and real image distributions.

---

### 6. Mode Score

**Formula:**
No standard formula; typically defined as:
<img width="784" height="64" alt="image" src="https://github.com/user-attachments/assets/7347c8a3-7e50-415c-a769-c5dcb7167838" />

- Modes are often identified using clustering (e.g., k-means) in feature space or by evaluating coverage of discrete classes.

**Explanation:**
Mode Score measures the number of distinct modes (or clusters) in the generated data distribution compared to the real data, assessing the GAN’s ability to avoid mode collapse.

**When, Where, and Why It Is Used:**
- **When**: Used to evaluate diversity and mode coverage in GANs.
- **Where**: Common in datasets with known modes (e.g., multi-class datasets like CIFAR-10).
- **Why**: Directly addresses mode collapse, a common GAN failure where the model generates limited variations.
- **Role**: Specialized metric for assessing diversity in generated samples.

**Advantages:**
- Explicitly measures mode collapse.
- Useful for datasets with clear modes or classes.

**Limitations:**
- Requires defining or estimating modes, which can be subjective.
- Not standardized, leading to implementation variations.
- Less comprehensive than FID or Precision/Recall.

**Example Use Case:**
In a GAN trained on MNIST digits, Mode Score evaluates whether all digit classes (0–9) are represented in the generated samples.

---

### 7. AM Score (Assessment Measure)

**Formula:**
No standard formula; often a combination of metrics (e.g., IS, FID, or custom measures) tailored to specific tasks. For example:
<img width="409" height="70" alt="image" src="https://github.com/user-attachments/assets/c8fc788f-893f-4414-9504-93229bef02b6" />

- $\( w_1, w_2 \)$: Weights for combining metrics.

**Explanation:**
AM Score is a composite metric that combines multiple evaluation measures (e.g., IS for quality, FID for distribution similarity) to provide a balanced assessment of GAN performance.

**When, Where, and Why It Is Used:**
- **When**: Used when a single metric is insufficient to capture GAN performance.
- **Where**: Common in research or applications requiring customized evaluation.
- **Why**: Allows flexibility to balance quality, diversity, and other factors specific to the task.
- **Role**: Ad-hoc metric for comprehensive GAN evaluation.

**Advantages:**
- Flexible, allowing task-specific combinations.
- Can balance multiple aspects of performance.

**Limitations:**
- Non-standardized, making comparisons difficult.
- Requires careful selection and weighting of component metrics.
- Subjective and task-dependent.

**Example Use Case:**
In evaluating a GAN for art generation, AM Score might combine FID (for realism) and a diversity metric to assess both quality and variety.

---

### 8. Parzen Window Log-Likelihood

**Formula:**
<img width="974" height="478" alt="image" src="https://github.com/user-attachments/assets/84175cee-65ab-4b80-bea1-7f811649dc0b" />

**Explanation:**
Parzen Window Log-Likelihood estimates the likelihood of real data under the generated distribution using kernel density estimation, measuring how well the GAN models the real data distribution.

**When, Where, and Why It Is Used:**
- **When**: Used to evaluate the likelihood of real data under the generated distribution.
- **Where**: Common in early GAN research or for probabilistic generative models.
- **Why**: Provides a probabilistic measure of how well the generated distribution matches the real data.
- **Role**: Assesses distribution fit, though less common in modern GAN evaluation.

**Advantages:**
- Theoretically grounded in density estimation.
- Captures distribution similarity.

**Limitations:**
- Highly sensitive to the choice of kernel bandwidth (\( \sigma \)).
- Computationally expensive for high-dimensional data (e.g., images).
- Poor performance in high dimensions due to curse of dimensionality.
- Largely replaced by FID and other metrics.

**Example Use Case:**
In early GAN research, Parzen Window Log-Likelihood was used to evaluate how well a GAN’s generated images match the real image distribution, though it’s now rarely used due to its limitations.

---

### Summary Table

| Metric                     | Focus                              | Task Type                  | Advantages                          | Limitations                         |
|----------------------------|------------------------------------|----------------------------|-------------------------------------|-------------------------------------|
| Inception Score (IS)       | Quality and diversity             | Image generation           | Simple, captures quality/diversity  | Classifier-dependent, insensitive to mode collapse |
| FID                        | Distribution similarity           | Image generation           | Robust, correlates with perception  | Inception-dependent, assumes Gaussian features |
| Precision/Recall           | Quality vs. diversity             | Image generation           | Separates quality and diversity     | Heuristic, computationally heavy    |
| Kernel Inception Distance  | Distribution similarity           | Image generation           | Less biased for small samples       | Kernel-dependent, less adopted      |
| Wasserstein Distance       | Distribution distance             | GAN training/evaluation    | Theoretically robust                | Computationally complex, approximate |
| Mode Score                 | Mode coverage                    | Image generation           | Directly addresses mode collapse    | Subjective, non-standardized        |
| AM Score                   | Combined quality/diversity        | Custom GAN evaluation      | Flexible, task-specific             | Non-standardized, subjective        |
| Parzen Window Log-Likelihood | Density estimation              | Probabilistic evaluation    | Probabilistic measure               | Bandwidth-sensitive, poor in high dimensions |

---

### Conclusion
Each GAN evaluation metric serves a specific purpose in assessing the quality, diversity, and fidelity of generated samples. FID is the most widely used due to its robustness and correlation with human perception, while Precision/Recall provides detailed insights into quality-diversity trade-offs. IS is simple but limited, and KID is a less biased alternative to FID. Wasserstein Distance is critical in training Wasserstein GANs, while Mode Score directly addresses mode collapse. AM Score allows customized evaluation, and Parzen Window Log-Likelihood, though outdated, offers a probabilistic perspective. Choosing the right metric depends on the task, dataset, and whether quality, diversity, or distribution fit is prioritized. For modern GAN evaluation, FID and Precision/Recall are typically recommended as primary metrics.
