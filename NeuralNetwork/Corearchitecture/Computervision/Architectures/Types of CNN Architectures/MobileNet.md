# **MobileNet (v1 â†’ v3): Complete Guide**  
### *Lightweight, Efficient, and Deployable CNNs for Mobile & Edge Devices*

---

## **Overview**

| Version | Year | Key Innovation | Top-1 (ImageNet) | Params | FLOPs |
|--------|------|----------------|------------------|--------|-------|
| **MobileNet-v1** | 2017 | **Depthwise Separable Conv** | 70.6% | **4.2M** | 0.57B |
| **MobileNet-v2** | 2018 | **Inverted Residuals + Linear Bottleneck** | 72.0% | **3.4M** | 0.30B |
| **MobileNet-v3** | 2019 | **Neural Architecture Search (NAS) + h-swish + Squeeze-and-Excitation** | **75.2%** | **5.4M** | 0.22B |

> **MobileNet = Accuracy Ã— Efficiency**  
> **Designed for**: Phones, IoT, embedded systems

---

## **1. MobileNet-v1: Depthwise Separable Convolution**

### **Standard Conv**
```
3Ã—3Ã—C_inÃ—C_out â†’ 9 Ã— C_in Ã— C_out ops
```

### **Depthwise Separable = 2 Steps**

| Step | Operation | Cost |
|------|---------|------|
| **Depthwise** | 3Ã—3 per channel | `9 Ã— C_in` |
| **Pointwise** | 1Ã—1 to mix | `C_in Ã— C_out` |

> **Total**: `9Ã—C_in + C_inÃ—C_out` â†’ **~8â€“9Ã— fewer ops**

### **Architecture (v1)**

| Layer | Type | Output | Stride |
|------|------|--------|--------|
| conv1 | 3Ã—3, 32 | 112Ã—112Ã—32 | 2 |
| **DW Block Ã—13** | DW + PW | â†“ | 1 or 2 |
| Avg Pool | 7Ã—7 | 1Ã—1Ã—1024 | |
| FC | | 1000 | |

> **Width Multiplier (Î±)**: Scale channels (e.g., Î±=0.5 â†’ 1M params)  
> **Resolution Multiplier (Ï)**: Input size (e.g., 128Ã—128)

---

## **2. MobileNet-v2: Inverted Residuals + Linear Bottleneck**

### **Problems with v1**
- **ReLU on low-dim â†’ information loss**
- **No skip connections**

### **Inverted Residual Block**

```text
Input (low-dim)
â”‚
â”œâ”€â–º 1Ã—1 conv â†’ expand (6Ã—) â†’ ReLU6
â”œâ”€â–º 3Ã—3 depthwise â†’ ReLU6
â””â”€â–º 1Ã—1 conv â†’ compress â†’ **NO ReLU** (linear)
â”‚
+â”€â”€â–º Skip (if stride=1 & same dim)
```

> **Expansion â†’ Depthwise â†’ Projection (linear)**

### **Why Linear Bottleneck?**
- ReLU on low-dim tensors â†’ **destroys information**
- Keep output **linear** â†’ preserves features

### **Architecture (v2)**

| Layer | t (expand) | c (out) | n (repeat) | s |
|------|------------|---------|------------|---|
| conv1 | - | 32 | 1 | 2 |
| bottleneck | 1 | 16 | 1 | 1 |
| bottleneck | 6 | 24 | 2 | 2 |
| bottleneck | 6 | 32 | 3 | 2 |
| bottleneck | 6 | 64 | 4 | 2 |
| bottleneck | 6 | 96 | 3 | 1 |
| bottleneck | 6 | 160 | 3 | 2 |
| bottleneck | 6 | 320 | 1 | 1 |
| conv2 | - | 1280 | 1 | 1 |
| AvgPool + FC | | 1000 | | |

> **Fewer params (3.4M)**, **faster**, **better accuracy**

---

## **3. MobileNet-v3: NAS + h-swish + SE**

### **Neural Architecture Search (NAS)**
- Used **MnasNet-style platform-aware NAS**
- Optimize for **latency**, not just FLOPs


---

# ðŸ§  Why MobileNet Exists

After **Xception**, people realized:

> Depthwise separable convolutions are super efficient â€”
> so why not build a *whole CNN* around them?

Thatâ€™s what **MobileNet** does.
Itâ€™s designed to run **on phones, embedded devices, and edge systems** â€” fast, light, and low power ðŸ”‹.

---

# ðŸš€ MOBILE NET v1 (2017)

### ðŸ§© The Core Idea â€” â€œDepthwise Separable Convolutionâ€

Instead of using normal convolution, every conv layer is split into:
1ï¸âƒ£ **Depthwise convolution:** looks at each channel separately
2ï¸âƒ£ **Pointwise (1Ã—1) convolution:** combines them

This is **exactly like Xception**, but used systematically.

---

### âš™ï¸ Structure

```
Input
â†“
Conv 3Ã—3 (normal)
â†“
Depthwise Conv + Pointwise Conv â†’ ReLU6
â†“
Depthwise Conv + Pointwise Conv â†’ ReLU6
â†“
... repeat ...
â†“
AvgPool + FC â†’ Softmax
```

---

### ðŸ’¡ Two Hyperparameters

| Parameter                 | Meaning                    | Example               |
| ------------------------- | -------------------------- | --------------------- |
| Î± (width multiplier)      | Shrinks number of channels | 0.75Ã— or 0.5Ã— smaller |
| Ï (resolution multiplier) | Shrinks input size         | 224Ã—224 â†’ 160Ã—160     |

âœ… Trade accuracy for speed & size
âœ… Used to tune model for devices

---

### ðŸ§® Efficiency

| Operation | Normal Conv | Depthwise Separable | Reduction   |
| --------- | ----------- | ------------------- | ----------- |
| Params    | (kÃ—kÃ—MÃ—N)   | (kÃ—kÃ—M + MÃ—N)       | ~9Ã— fewer   |
| FLOPs     | High        | Very low            | 8â€“9Ã— faster |

---

### ðŸ”‹ Results

| Model            | Accuracy (ImageNet) | Params | Speed  |
| ---------------- | ------------------- | ------ | ------ |
| VGG16            | 71%                 | 138M   | Slow   |
| **MobileNet-v1** | 70â€“71%              | 4.2M   | âš¡ Fast |

âœ… Runs easily on mobile CPUs
âœ… Simple, lightweight backbone

---

# âš™ï¸ MOBILE NET v2 (2018)

Google refined the design to fix some v1 limitations.

---

## ðŸ§© Problem in v1

Depthwise convs were great â€” but as we made the network deeper, features became **too compressed** (information lost).

---

## ðŸ’¡ Solution: â€œInverted Residuals + Linear Bottleneckâ€

Letâ€™s break that down simply ðŸ‘‡

---

### 1ï¸âƒ£ Normal Residual Block (ResNet style)

```
Input â†’ Conv â†’ Conv â†’ Add(Input)
```

### 2ï¸âƒ£ MobileNet-v2 Block (Inverted Residual)

```
Input â†’ 1Ã—1 (expand channels) â†’ Depthwise Conv â†’ 1Ã—1 (project down) â†’ Add(Input)
```

So it first **expands**, then **compresses** â€” the *opposite* of ResNet (hence â€œinvertedâ€).

---

### ðŸ§® Linear Bottleneck

Normally, CNNs end layers with ReLU activation.
But when compressing (reducing channels), **ReLU kills information**.
So v2 keeps the **bottleneck layer linear** (no ReLU there).

Thatâ€™s why itâ€™s called **Linear Bottleneck** â€” preserves fine details.

---

### ðŸ§  Intuitive Example

Imagine squeezing toothpaste (features) out of a tube (bottleneck).
If you apply a ReLU (cutting negative values), half the paste is gone!
So MobileNet-v2 doesnâ€™t squeeze too hard and avoids losing info.

---

### ðŸ§© MobileNet-v2 Block

```
Input (small channels)
â†“
1Ã—1 Conv â†’ Expand (6Ã— channels)
â†“
Depthwise Conv
â†“
1Ã—1 Conv â†’ Project back (Linear)
â†“
Add skip connection (if same size)
```

---

### âœ… Why Itâ€™s Better

| Feature      | MobileNet-v1        | MobileNet-v2         |
| ------------ | ------------------- | -------------------- |
| Block type   | Depthwise separable | Inverted residual    |
| ReLU usage   | Everywhere          | Linear at bottleneck |
| Feature loss | High                | Preserves details    |
| Speed        | Fast                | Fast                 |
| Accuracy     | ~71%                | ~74%                 |

---

# ðŸš€ MOBILE NET v3 (2019)

MobileNet-v3 is the **best** and **smartest** version.
It was designed using **Neural Architecture Search (NAS)** â€” meaning Googleâ€™s AI *automatically discovered the best design*.

---

## ðŸ’¡ New Additions

1ï¸âƒ£ **SE Blocks (Squeeze-and-Excitation)**
â†’ Adds a small attention mechanism
â†’ Learns which channels are important
â†’ "Channel attention"

2ï¸âƒ£ **Swish / h-swish Activation**
â†’ Smoother than ReLU
â†’ Helps gradients flow better

[
h_swish(x) = x * ReLU6(x+3) / 6
]

3ï¸âƒ£ **Smarter block design**
â†’ Uses both v1 and v2 ideas
â†’ Mix of 3Ã—3 and 5Ã—5 depthwise convs
â†’ SE + Linear bottleneck in every block

---

### ðŸ§© Block Example (Simplified)

```
Input
â†“
1Ã—1 Conv â†’ Expand
â†“
Depthwise Conv (3Ã—3 or 5Ã—5)
â†“
SE Block (channel attention)
â†“
1Ã—1 Conv â†’ Project (Linear)
â†“
h-swish Activation
```

---

### ðŸ§  Two Versions

| Model                  | Optimized For     | Used In                  |
| ---------------------- | ----------------- | ------------------------ |
| MobileNet-v3 **Small** | Low-power devices | Phones, microcontrollers |
| MobileNet-v3 **Large** | High-performance  | Cloud + Mobile GPUs      |

---

### ðŸ“Š Performance

| Model                  | Params | Top-1 Acc  | Speed       |
| ---------------------- | ------ | ---------- | ----------- |
| MobileNet-v1           | 4.2M   | 71%        | âš¡ Fast      |
| MobileNet-v2           | 3.4M   | 74%        | âš¡âš¡ Faster   |
| **MobileNet-v3 Large** | 5.4M   | **76â€“78%** | âš¡âš¡âš¡ Fastest |

âœ… Combines Xception-like efficiency
âœ… Adds ResNet-style skip connections
âœ… Adds SE and better activations

---

# ðŸ§© Evolution Summary

| Version | Core Idea                              | Key Feature      | Analogy                   |
| ------- | -------------------------------------- | ---------------- | ------------------------- |
| **v1**  | Depthwise separable conv               | Simplicity       | â€œLightweight Inceptionâ€   |
| **v2**  | Inverted residuals + Linear bottleneck | Better info flow | â€œResNet-style efficiencyâ€ |
| **v3**  | NAS + SE + h-swish                     | Smart & adaptive | â€œAI-optimized hybridâ€     |

---

# ðŸ§  Simple Analogy Recap

| Model            | You Can Think Of It As          |
| ---------------- | ------------------------------- |
| **Inception**    | Looking at image in many ways   |
| **Xception**     | Doing it smartly â€” per channel  |
| **MobileNet-v1** | Doing it smartly *and fast*     |
| **MobileNet-v2** | Fast, but avoids losing details |
| **MobileNet-v3** | Fast + smart + AI-tuned         |

---

# âœ… TL;DR Summary

| Version | Main Innovation                        | Why It Matters           |
| ------- | -------------------------------------- | ------------------------ |
| **v1**  | Depthwise separable conv               | Huge speed-up            |
| **v2**  | Inverted residuals + linear bottleneck | Preserve info            |
| **v3**  | SE + h-swish + NAS                     | Smarter, higher accuracy |

---




### **Key Innovations**

| Innovation | Description |
|----------|-----------|
| **h-swish** | `x * ReLU6(x+3)/6` â†’ better than ReLU6 |
| **Squeeze-and-Excitation (SE)** | Channel attention |
| **Hard-swish & Hard-sigmoid** | Quantization-friendly |
| **Redesigned Layers** | Remove expensive early layers |
| **Last Stage Redesign** | 1Ã—1 â†’ 5Ã—5 â†’ SE |

### **Architecture (v3-Large)**

| Layer | Operator | Exp | Out | SE | NL | s |
|------|----------|-----|-----|----|----|---|
| conv | conv2d | - | 16 | - | H | 2 |
| bneck | bneck, 3Ã—3 | 16 | 16 | - | R | 1 |
| bneck | bneck, 3Ã—3 | 64 | 24 | - | R | 2 |
| bneck | bneck, 3Ã—3 | 72 | 24 | - | R | 1 |
| bneck | bneck, 5Ã—5 | 72 | 40 | Yes | R | 2 |
| bneck | bneck, 5Ã—5 | 120 | 40 | Yes | R | 1 |
| bneck | bneck, 5Ã—5 | 240 | 80 | - | H | 2 |
| bneck | bneck, 3Ã—3 | 200 | 80 | - | H | 1 |
| bneck | bneck, 3Ã—3 | 184 | 80 | - | H | 1 |
| bneck | bneck, 3Ã—3 | 184 | 80 | - | H | 1 |
| bneck | bneck, 3Ã—3 | 480 | 112 | Yes | H | 1 |
| bneck | bneck, 5Ã—5 | 672 | 160 | Yes | H | 2 |
| bneck | bneck, 5Ã—5 | 960 | 160 | Yes | H | 1 |
| conv | 1Ã—1 | - | 960 | - | H | 1 |
| pool + conv | | 1280 | | | |
| conv | 1Ã—1 | - | k | - | - | |

> **NL**: `R` = ReLU, `H` = h-swish

---

## **4. Full PyTorch Code: MobileNet-v3 (Small & Large)**

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
num_classes = 10
batch_size = 128
learning_rate = 0.05
num_epochs = 100

# Data
transform_train = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

transform_test = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)

# h-swish
class HSwish(nn.Module):
    def forward(self, x):
        return x * nn.functional.relu6(x + 3, inplace=True) / 6

# SE Module
class SEModule(nn.Module):
    def __init__(self, channels, reduction=4):
        super().__init__()
        self.fc = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1),
            nn.Hardsigmoid(inplace=True)
        )

    def forward(self, x):
        return x * self.fc(x)

# MobileNet-v3 Block
class MBConv(nn.Module):
    def __init__(self, in_c, out_c, exp_c, kernel, stride, se, nl):
        super().__init__()
        self.use_se = se
        self.use_skip = stride == 1 and in_c == out_c

        # Expand
        if exp_c != in_c:
            self.expand = nn.Sequential(
                nn.Conv2d(in_c, exp_c, 1, bias=False),
                nn.BatchNorm2d(exp_c),
                HSwish() if nl == 'H' else nn.ReLU6(inplace=True)
            )
        else:
            self.expand = None

        # Depthwise
        self.depthwise = nn.Sequential(
            nn.Conv2d(exp_c, exp_c, kernel, stride, kernel//2, groups=exp_c, bias=False),
            nn.BatchNorm2d(exp_c),
            HSwish() if nl == 'H' else nn.ReLU6(inplace=True)
        )

        # SE
        if self.use_se:
            self.se = SEModule(exp_c)

        # Project
        self.project = nn.Sequential(
            nn.Conv2d(exp_c, out_c, 1, bias=False),
            nn.BatchNorm2d(out_c)
        )

    def forward(self, x):
        out = x
        if self.expand:
            out = self.expand(out)
        out = self.depthwise(out)
        if self.use_se:
            out = self.se(out)
        out = self.project(out)
        if self.use_skip:
            out = out + x
        return out

# MobileNet-v3
class MobileNetV3(nn.Module):
    def __init__(self, mode='large', num_classes=10):
        super().__init__()
        configs = {
            'small': [
                # k, exp, out, se, nl, s
                [3, 16, 16, True, 'R', 2],
                [3, 72, 24, False, 'R', 2],
                [3, 88, 24, False, 'R', 1],
                [5, 96, 40, True, 'H', 2],
                [5, 240, 40, True, 'H', 1],
                [5, 240, 40, True, 'H', 1],
                [5, 120, 48, True, 'H', 1],
                [5, 144, 48, True, 'H', 1],
                [5, 288, 96, True, 'H', 2],
                [5, 576, 96, True, 'H', 1],
                [5, 576, 96, True, 'H', 1],
            ],
            'large': [
                [3, 16, 16, False, 'R', 1],
                [3, 64, 24, False, 'R', 2],
                [3, 72, 24, False, 'R', 1],
                [5, 72, 40, True, 'R', 2],
                [5, 120, 40, True, 'R', 1],
                [5, 120, 40, True, 'R', 1],
                [3, 240, 80, False, 'H', 2],
                [3, 200, 80, False, 'H', 1],
                [3, 184, 80, False, 'H', 1],
                [3, 184, 80, False, 'H', 1],
                [3, 480, 112, True, 'H', 1],
                [3, 672, 112, True, 'H', 1],
                [5, 672, 160, True, 'H', 2],
                [5, 960, 160, True, 'H', 1],
                [5, 960, 160, True, 'H', 1],
            ]
        }

        cfg = configs[mode]
        self.stem = nn.Sequential(
            nn.Conv2d(3, 16, 3, 2, 1, bias=False),
            nn.BatchNorm2d(16),
            HSwish()
        )

        in_c = 16
        self.blocks = nn.ModuleList()
        for k, exp, out, se, nl, s in cfg:
            self.blocks.append(MBConv(in_c, out, exp, k, s, se, nl))
            in_c = out

        self.head = nn.Sequential(
            nn.Conv2d(in_c, 960 if mode == 'large' else 576, 1, bias=False),
            nn.BatchNorm2d(960 if mode == 'large' else 576),
            HSwish(),
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(960 if mode == 'large' else 576, 1280 if mode == 'large' else 1024, 1),
            HSwish(),
            nn.Conv2d(1280 if mode == 'large' else 1024, num_classes, 1)
        )

    def forward(self, x):
        x = self.stem(x)
        for block in self.blocks:
            x = block(x)
        x = self.head(x)
        return x.squeeze(-1).squeeze(-1)

# Model
model = MobileNetV3(mode='large', num_classes=num_classes).to(device)

# Loss & Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# Training & Test
def train():
    model.train()
    for epoch in range(num_epochs):
        for i, (images, labels) in enumerate(trainloader):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        scheduler.step()

def test():
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Accuracy: {100 * correct / total:.2f}%')

print("Training MobileNet-v3 Large...")
train()
print("Testing...")
test()
```

---

## **5. Pretrained Models (PyTorch)**

```python
import torchvision.models as models

# MobileNet-v2
model = models.mobilenet_v2(pretrained=True)

# MobileNet-v3
model = models.mobilenet_v3_large(pretrained=True)
model = models.mobilenet_v3_small(pretrained=True)
```

---

## **6. Performance Summary**

| Model | Params | FLOPs | Top-1 | Latency (ms) |
|------|--------|-------|-------|--------------|
| **v1** | 4.2M | 0.57B | 70.6% | ~20 |
| **v2** | 3.4M | 0.30B | 72.0% | ~15 |
| **v3-Large** | 5.4M | 0.22B | **75.2%** | **~12** |
| **v3-Small** | 2.9M | 0.06B | 67.4% | ~8 |

---

## **7. Efficiency Chart (Chart.js)**
<img width="1014" height="557" alt="image" src="https://github.com/user-attachments/assets/2a1156b8-7a9c-4b15-8df4-f06cecfdc5d0" />

---

## **8. Summary**

| Version | Best For | Key Idea |
|--------|---------|---------|
| **v1** | First mobile CNN | Depthwise Separable |
| **v2** | Efficiency + Accuracy | Inverted Residuals |
| **v3** | State-of-the-art mobile | NAS + h-swish + SE |

---

## **Run Code**

```bash
pip install torch torchvision
python mobilenet_v3.py
```

> Expected: **~95%+ on CIFAR-10**

---

**MobileNet is the backbone of on-device AI**  
Used in:
- **TensorFlow Lite**
- **Edge TPU**
- **Android Neural Networks API**
- **Object detection (SSD)**
- **Segmentation**

---
