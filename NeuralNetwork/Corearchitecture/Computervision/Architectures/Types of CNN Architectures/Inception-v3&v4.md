
### *Inception Evolution â€” From GoogLeNet to State-of-the-Art Efficiency*

---

## **Overview**

| Model | Year | Key Innovations | Top-5 Error (ImageNet) | Params |
|------|------|------------------|------------------------|--------|
| **Inception-v1 (GoogLeNet)** | 2014 | 1Ã—1, 3Ã—3, 5Ã—5 parallel | 6.67% | 6.8M |
| **Inception-v2/v3** | 2015 | Factorized convolutions, BatchNorm, Label Smoothing | **5.6%** | ~25M |
| **Inception-v4** | 2016 | Residual + Inception, Stem redesign | **4.9%** | ~43M |

> **Inception-v3** = *Efficient + Accurate*  
> **Inception-v4** = *Inception + Residual = Best of both worlds*

---

## **1. Inception-v3: Key Innovations**

| Innovation | Description |
|----------|-----------|
| **Factorized 7Ã—7** | Replace 7Ã—7 â†’ two 3Ã—3 â†’ **75% fewer params** |
| **Asymmetric Factorization** | 5Ã—5 â†’ 1Ã—5 + 5Ã—1 â†’ **33% cheaper** |
| **Efficient Grid Reduction** | Parallel pooling + stride conv â†’ no info loss |
| **Batch Normalization** | After every conv |
| **Label Smoothing** | Regularization: `y = 0.9 * true + 0.1 / K` |
| **Auxiliary Classifier** | Removed (not needed with BN) |

---

## **2. Inception-v3 Architecture**

| Layer | Type | Output | Details |
|------|------|--------|-------|
| **Stem** | | 35Ã—35Ã—384 | 3Ã—3 â†’ 3Ã—3 â†’ 3Ã—3 + parallel paths |
| **Inception-A** Ã—4 | | 35Ã—35Ã—384 | 1Ã—1, 3Ã—3, 5Ã—5 |
| **Reduction-A** | | 17Ã—17Ã—1024 | Grid reduction |
| **Inception-B** Ã—7 | | 17Ã—17Ã—1024 | Asymmetric + factorized |
| **Reduction-B** | | 8Ã—8Ã—1536 | Pool + stride |
| **Inception-C** Ã—3 | | 8Ã—8Ã—1536 | 1Ã—1, 1Ã—3+3Ã—1 |
| **Global Avg Pool** | | 1Ã—1Ã—2048 | |
| **FC + Softmax** | | 1000 | |

> **Total**: **42 layers**, **~25M params**

---

## **3. Inception-v4: Residual + Inception**

### **Inception-ResNet Block**
```text
Input
â”‚
â”œâ”€â”€ 1Ã—1
â”œâ”€â”€ 1Ã—1 â†’ 3Ã—3
â”œâ”€â”€ 1Ã—1 â†’ 3Ã—3 â†’ 3Ã—3
â”‚
â””â”€â–º Concat â†’ 1Ã—1 (scale) â†’ + Input
```

### **Inception-v4 vs v3**

| Feature | v3 | v4 |
|-------|----|----|
| Residual Connections | No | Yes |
| Stem | Complex | Cleaner |
| Depth | 42 | 75 |
| Params | 25M | 43M |
| Top-5 Error | 5.6% | **4.9%** |

---

## **4. Full PyTorch Code: Inception-v3 (from `torchvision`)**

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
num_classes = 10
batch_size = 64
learning_rate = 0.045  # As in paper
num_epochs = 100
label_smoothing = 0.1

# Data (CIFAR-10 â†’ 224Ã—224)
transform_train = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

transform_test = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)

# Load Inception-v3
model = torchvision.models.inception_v3(pretrained=False, aux_logits=True, num_classes=num_classes)
model = model.to(device)

# Label Smoothing Loss
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, pred, target):
        confidence = 1.0 - self.smoothing
        log_probs = torch.nn.functional.log_softmax(pred, dim=-1)
        n_classes = pred.size(-1)
        with torch.no_grad():
            true_dist = torch.zeros_like(log_probs)
            true_dist.fill_(self.smoothing / (n_classes - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), confidence)
        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))

criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)
optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# Training
def train():
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, (images, labels) in enumerate(trainloader):
            images, labels = images.to(device), labels.to(device)

            if model.aux_logits:
                outputs, aux_outputs = model(images)
                loss1 = criterion(outputs, labels)
                loss2 = criterion(aux_outputs, labels)
                loss = loss1 + 0.4 * loss2
            else:
                outputs = model(images)
                loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if (i+1) % 100 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {loss.item():.4f}')

        scheduler.step()

# Test
def test():
    model.eval()
    model.aux_logits = False
    correct = total = 0
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Accuracy: {100 * correct / total:.2f}%')

print("Training Inception-v3 on CIFAR-10...")
train()
print("Testing...")
test()
```
Excellent â€” now youâ€™re entering the **evolved Inception family**, where Google took the original Inception (from GoogLeNet, 2014) and **made it faster, deeper, and smarter** using clever factorization and architectural tweaks.

Letâ€™s go step-by-step ðŸ‘‡
Youâ€™ll understand both **Inception-v3** and **Inception-v4** â€” simply, logically, and with enough depth to connect to real implementation or interview discussion.

---

# ðŸ§  Why Inception Needed to Evolve

Original **GoogLeNet (Inception-v1)** was brilliant â€” it captured **multi-scale features** efficiently.
But as models got deeper:

* Computation grew heavy
* Some filters (like 5Ã—5) became too expensive
* Gradient flow could still improve

So, Inception-v2 â†’ v3 â†’ v4 were **progressive refinements** to fix these issues.

---

# ðŸš€ INCEPTION-v2 / v3 â€” â€œFactorizationâ€ and Efficiency

## ðŸŽ¯ Main Goal:

Make convolutions **faster and more efficient**
without losing accuracy or depth.

---

## 1ï¸âƒ£ **Factorization of Convolutions**

Instead of a 5Ã—5 convolution (expensive), they replaced it with two 3Ã—3 convolutions.

### Why?

Parameter cost:

* 5Ã—5 with (C_{in} = C_{out} = 256):
  (256 Ã— 256 Ã— 5 Ã— 5 = 1.6M)
* Two 3Ã—3 layers:
  (256 Ã— 256 Ã— 3 Ã— 3 Ã— 2 = 1.18M)

âœ… **Fewer parameters, same receptive field**

---

## 2ï¸âƒ£ **Asymmetric Convolutions**

Instead of a 3Ã—3, use **1Ã—3 followed by 3Ã—1**.

[
3Ã—3 \Rightarrow (1Ã—3) + (3Ã—1)
]

### Why?

* 3Ã—3: 9 parameters per input-output channel pair
* 1Ã—3 + 3Ã—1: 6 total parameters
  âœ… 33% fewer parameters
  âœ… More nonlinearity (ReLU between them)

---

## 3ï¸âƒ£ **Batch Normalization Everywhere**

Every conv layer is followed by BN â†’ ReLU
â†’ stabilizes and speeds up training.

---

## 4ï¸âƒ£ **Auxiliary Classifiers = Regularization**

Same as v1, but improved with BN.

---

## ðŸ§© Inception-v3 Block Types

Inception-v3 organizes the network into **three block families**:

| Block       | Purpose                           | Example Filter Sizes       |
| ----------- | --------------------------------- | -------------------------- |
| Inception-A | Normal multi-scale block          | 1Ã—1, 3Ã—3, 5Ã—5 (factorized) |
| Inception-B | Reduces spatial size (downsample) | 3Ã—3 stride 2               |
| Inception-C | Deeper, uses asymmetric convs     | 1Ã—3, 3Ã—1                   |

---

## ðŸ—ï¸ Simplified Architecture Overview (Inception-v3)

| Stage               | Block Type  | Output Size (approx.) |
| ------------------- | ----------- | --------------------- |
| Stem                | Conv + Pool | 149Ã—149               |
| Inception-A Ã— 3     | â€”           | 35Ã—35                 |
| Reduction-A         | Downsample  | 17Ã—17                 |
| Inception-B Ã— 5     | â€”           | 17Ã—17                 |
| Reduction-B         | Downsample  | 8Ã—8                   |
| Inception-C Ã— 2     | â€”           | 8Ã—8                   |
| Global AvgPool + FC | â€”           | 1Ã—1 â†’ Classes         |

---

## ðŸ§® Mathematical Summary

For each Inception-v3 module:
[
y = \text{concat}(f_{1Ã—1}(x), f_{1Ã—3â†’3Ã—1}(x), f_{3Ã—3â†’3Ã—3}(x), f_{poolâ†’1Ã—1}(x))
]
where each ( f ) branch uses BN + ReLU between layers.

---

## âœ… Result

* **~28M parameters**
* **Factorized convolutions**
* **Better gradient flow**
* **State-of-the-art accuracy (ImageNet 2015)**

---

# ðŸ§© INCEPTION-v4 â€” When Inception Meets ResNet (2016)

Now Google said â€”

> â€œWhat if we combine the *multi-scale idea of Inception* with the *residual idea of ResNet*?â€

Hence two major architectures were born:

* **Inception-v4** â€” pure Inception, deeper and cleaner
* **Inception-ResNet-v2** â€” hybrid Inception + residual connections

---

## ðŸ§  Key Innovations in Inception-v4

1ï¸âƒ£ **Modular Design (4 building blocks)**

| Block         | Function                       |
| ------------- | ------------------------------ |
| Stem          | Initial convolutions + pooling |
| Inception-A   | Multi-scale feature extraction |
| Inception-B   | Factorized 7Ã—7 convs           |
| Inception-C   | High-dimensional feature mix   |
| Reduction-A/B | Downsampling blocks            |

---

2ï¸âƒ£ **Deeper and More Symmetrical**

* v4 has **more Inception blocks** than v3
* Every block follows a clean, repeated pattern
* Used **â€œsameâ€ padding** for stable spatial sizes

---

3ï¸âƒ£ **Stronger Regularization**

* Label smoothing
* Dropout
* BatchNorm
  â†’ makes deeper networks trainable

---

## ðŸ§® Inception-v4 Block Details

| Block       | Core Design                        |
| ----------- | ---------------------------------- |
| Inception-A | Uses 1Ã—1, 3Ã—3, 5Ã—5 branches        |
| Reduction-A | Mix of stride=2 convs and pooling  |
| Inception-B | Factorized 7Ã—7 convs (1Ã—7 + 7Ã—1)   |
| Reduction-B | Another downsample stage           |
| Inception-C | Final stage with multiple 1Ã—3, 3Ã—1 |

---

## ðŸ§© Inception-v4 Architecture Summary

| Stage           | Type        | Output Size |
| --------------- | ----------- | ----------- |
| Stem            | Conv + Pool | 149Ã—149     |
| Inception-A Ã— 4 | â€”           | 35Ã—35       |
| Reduction-A     | â†“           | 17Ã—17       |
| Inception-B Ã— 7 | â€”           | 17Ã—17       |
| Reduction-B     | â†“           | 8Ã—8         |
| Inception-C Ã— 3 | â€”           | 8Ã—8         |
| Global AvgPool  | â€”           | 1Ã—1         |
| FC              | â€”           | num_classes |

---

## âš–ï¸ Inception-v4 vs Inception-ResNet-v2

| Feature           | Inception-v4   | Inception-ResNet-v2       |
| ----------------- | -------------- | ------------------------- |
| Skip Connections  | âŒ No           | âœ… Yes (Residual style)    |
| Speed             | Slower         | Faster                    |
| Parameters        | Higher         | Fewer                     |
| Gradient Flow     | Good           | Excellent                 |
| Accuracy          | Very high      | Slightly higher           |
| Design Philosophy | Pure Inception | Hybrid Inception + ResNet |

---

# ðŸ§® Factorization Example (Math)

Letâ€™s say we have a 7Ã—7 conv on 128 channels.
Original cost:
[
7Ã—7Ã—128Ã—128 = 802,816
]

Factorized as:
[
1Ã—7 + 7Ã—1
\Rightarrow (1Ã—7Ã—128Ã—128) + (7Ã—1Ã—128Ã—128) = 2Ã—114,688 = 229,376
]

âœ… ~3.5Ã— fewer parameters
âœ… Same receptive field
âœ… Extra nonlinearity

Thatâ€™s the **core mathematical innovation** of Inception-v3/v4.

---

# ðŸ§© Summary Table â€” Evolution of Inception

| Version          | Year | Key Idea               | Improvement            |
| ---------------- | ---- | ---------------------- | ---------------------- |
| v1 (GoogLeNet)   | 2014 | Multi-scale filters    | Efficient CNN          |
| v2               | 2015 | Factorized 5Ã—5 â†’ 2Ã—3Ã—3 | Faster, fewer params   |
| v3               | 2015 | Asymmetric convs, BN   | More accurate          |
| v4               | 2016 | Modular deep design    | Deeper, more efficient |
| Inception-ResNet | 2016 | Add residuals          | Combines best of both  |

---

# ðŸ§  Intuition Recap

| Analogy          | Meaning                                                     |
| ---------------- | ----------------------------------------------------------- |
| Inception-v1     | â€œLook at everything â€” small & large patterns.â€              |
| Inception-v3     | â€œDo it faster with smaller filters.â€                        |
| Inception-v4     | â€œStack it deeper, more organized.â€                          |
| Inception-ResNet | â€œDo all that, but with shortcut safety (skip connections).â€ |

---

---

# ðŸ§© **Inception-v3 â€” Modular Design**

Inception-v3 uses three main block types:

* **Inception-A** â†’ captures multi-scale local features (35Ã—35)
* **Inception-B** â†’ deeper middle layers with asymmetric 1Ã—7, 7Ã—1 convs (17Ã—17)
* **Inception-C** â†’ fine-grained, high-level patterns (8Ã—8)

---

## ðŸ§  1ï¸âƒ£ Inception-A Block (Factorized 5Ã—5 â†’ Two 3Ã—3)

```python
class InceptionA(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.branch1 = nn.Conv2d(in_channels, 64, kernel_size=1)

        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, 48, kernel_size=1),
            nn.Conv2d(48, 64, kernel_size=5, padding=2)  # factorized 5x5 in original
        )

        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=1),
            nn.Conv2d(64, 96, kernel_size=3, padding=1),
            nn.Conv2d(96, 96, kernel_size=3, padding=1)
        )

        self.branch4 = nn.Sequential(
            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),
            nn.Conv2d(in_channels, 32, kernel_size=1)
        )

    def forward(self, x):
        out1 = self.branch1(x)
        out2 = self.branch2(x)
        out3 = self.branch3(x)
        out4 = self.branch4(x)
        return torch.cat([out1, out2, out3, out4], dim=1)
```

### âž• Shape Example:

Input: `35Ã—35Ã—192`
Output channels: 64+64+96+32 = **256**
Output: `35Ã—35Ã—256`

**Why it works:** combines local (1Ã—1), medium (3Ã—3), and large (5Ã—5) receptive fields efficiently.

---

## ðŸ§  2ï¸âƒ£ Inception-B Block (Asymmetric Factorization)

```python
class InceptionB(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.branch1 = nn.Conv2d(in_channels, 192, kernel_size=1)

        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, 128, kernel_size=1),
            nn.Conv2d(128, 128, kernel_size=(1,7), padding=(0,3)),
            nn.Conv2d(128, 192, kernel_size=(7,1), padding=(3,0))
        )

        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, 128, kernel_size=1),
            nn.Conv2d(128, 128, kernel_size=(7,1), padding=(3,0)),
            nn.Conv2d(128, 128, kernel_size=(1,7), padding=(0,3)),
            nn.Conv2d(128, 192, kernel_size=(7,1), padding=(3,0)),
            nn.Conv2d(192, 192, kernel_size=(1,7), padding=(0,3))
        )

        self.branch4 = nn.Sequential(
            nn.AvgPool2d(3, stride=1, padding=1),
            nn.Conv2d(in_channels, 192, kernel_size=1)
        )

    def forward(self, x):
        return torch.cat([
            self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)
        ], dim=1)
```

### âž• Shape Example:

Input: `17Ã—17Ã—768`
Output: `17Ã—17Ã—768` (each branch ~192 channels)

**Why asymmetric convs?**
`(1Ã—7)` + `(7Ã—1)` = same receptive field as `7Ã—7`, but
âœ… fewer params
âœ… more nonlinearities
âœ… faster training

---

## ðŸ§  3ï¸âƒ£ Reduction Blocks (Downsampling)

To move from one grid size to the next (like 35Ã—35 â†’ 17Ã—17):

```python
class ReductionA(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.branch1 = nn.Conv2d(in_channels, 384, kernel_size=3, stride=2)
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=1),
            nn.Conv2d(64, 96, kernel_size=3, padding=1),
            nn.Conv2d(96, 96, kernel_size=3, stride=2)
        )
        self.branch3 = nn.MaxPool2d(kernel_size=3, stride=2)

    def forward(self, x):
        return torch.cat([
            self.branch1(x), self.branch2(x), self.branch3(x)
        ], dim=1)
```

This replaces the need for a simple pooling layer, preserving multi-path richness even when reducing spatial size.

---

## ðŸ—ï¸ Full Inception-v3 Outline

```
Input (299x299x3)
â†“
Stem (Conv + Pool)
â†“
Inception-A Ã— 3
â†“
Reduction-A
â†“
Inception-B Ã— 5
â†“
Reduction-B
â†“
Inception-C Ã— 2
â†“
AvgPool â†’ FC (1000 classes)
```

âœ… ~28M parameters
âœ… High accuracy
âœ… Efficient training

---

# ðŸ§© **Inception-v4 â€” Modular, Deeper, Cleaner**

Now letâ€™s see how **Inception-v4** expanded this concept.

---

## ðŸ§  1ï¸âƒ£ Stem Block (Initial Convolutions)

```python
class Stem(nn.Module):
    def __init__(self):
        super().__init__()
        self.stem = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2),   # 149x149
            nn.Conv2d(32, 32, kernel_size=3),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.MaxPool2d(3, stride=2),                   # 73x73
            nn.Conv2d(64, 80, kernel_size=1),
            nn.Conv2d(80, 192, kernel_size=3),
            nn.MaxPool2d(3, stride=2)                    # 35x35
        )

    def forward(self, x):
        return self.stem(x)
```

This gives us a solid 35Ã—35Ã—192 output â€” the same spatial start point as Inception-A.

---

## ðŸ§© 2ï¸âƒ£ Inception-A, Reduction-A, Inception-B, Reduction-B, Inception-C Blocks

Very similar logic to v3 but:

* **More filters per branch**
* **More layers per block**
* **Consistent â€œsame paddingâ€**
* **Better normalization and structure**

Each block follows the same concept:

* Multi-branch (1Ã—1, 3Ã—3, 5Ã—5) convs
* Parallel pooling + 1Ã—1 projection
* Concatenation
* Downsampling via reduction blocks

---

## ðŸ§® Example Block (Inception-C)

```python
class InceptionC(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.branch1 = nn.Conv2d(in_channels, 256, kernel_size=1)

        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, 384, kernel_size=1),
            nn.Conv2d(384, 256, kernel_size=(1,3), padding=(0,1)),
            nn.Conv2d(384, 256, kernel_size=(3,1), padding=(1,0))
        )

        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, 384, kernel_size=1),
            nn.Conv2d(384, 448, kernel_size=(3,1), padding=(1,0)),
            nn.Conv2d(448, 512, kernel_size=(1,3), padding=(0,1)),
            nn.Conv2d(512, 256, kernel_size=(1,3), padding=(0,1)),
            nn.Conv2d(512, 256, kernel_size=(3,1), padding=(1,0))
        )

        self.branch4 = nn.Sequential(
            nn.AvgPool2d(3, stride=1, padding=1),
            nn.Conv2d(in_channels, 256, kernel_size=1)
        )

    def forward(self, x):
        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)
```

---

## ðŸ§© Inception-v4 Summary Flow

| Stage          | Type        | Output Size | Description              |
| -------------- | ----------- | ----------- | ------------------------ |
| Stem           | Conv + Pool | 35Ã—35       | Initial                  |
| Inception-A Ã—4 | â€”           | 35Ã—35       | Early feature extraction |
| Reduction-A    | â†“           | 17Ã—17       | Downsample               |
| Inception-B Ã—7 | â€”           | 17Ã—17       | Deeper multi-scale       |
| Reduction-B    | â†“           | 8Ã—8         | Downsample               |
| Inception-C Ã—3 | â€”           | 8Ã—8         | Final deep features      |
| AvgPool + FC   | â€”           | 1Ã—1         | Classifier               |

âœ… Deeper (42 layers)
âœ… More balanced design
âœ… Extremely accurate on ImageNet

---

# âš–ï¸ **v3 vs v4 â€” Side-by-Side**

| Feature          | Inception-v3          | Inception-v4                      |
| ---------------- | --------------------- | --------------------------------- |
| Factorization    | 3Ã—3, 1Ã—7, 7Ã—1         | Extended (up to 1Ã—7Ã—1Ã—3 combos)   |
| Normalization    | BN                    | BN + Better initialization        |
| Depth            | ~48 layers            | ~75 layers                        |
| Reduction design | Simpler               | Two dedicated reduction blocks    |
| Accuracy         | ~78% top-1 (ImageNet) | ~80%+                             |
| Speed            | Faster                | Slightly slower but more accurate |

---

# ðŸ§® Math Summary â€” Factorization Efficiency

| Kernel | Original Params | Factorized Params | Ratio      |
| ------ | --------------- | ----------------- | ---------- |
| 5Ã—5    | 25              | 9+9 = 18          | 1.4Ã— fewer |
| 7Ã—7    | 49              | 7+7 = 14          | 3.5Ã— fewer |

So factorization saves parameters and increases nonlinearity (extra BN + ReLU).

---

# ðŸŽ¯ Final Takeaway

| Concept              | Meaning                                            |
| -------------------- | -------------------------------------------------- |
| **Inception-v1**     | Multi-scale filters                                |
| **Inception-v2/v3**  | Factorization for speed & depth                    |
| **Inception-v4**     | Deep, modular, clean with heavy reuse              |
| **Inception-ResNet** | Inception + skip connections = best of both worlds |

---



---

## **5. Inception-v3 Factorization Examples**

### **5Ã—5 â†’ 1Ã—5 + 5Ã—1**
```text
Input â†’ 1Ã—5 conv â†’ 5Ã—1 conv â†’ Output
```
â†’ **33% cheaper**

### **7Ã—7 â†’ 1Ã—7 + 7Ã—1 â†’ two 3Ã—3**
```text
7Ã—7 = 49 ops
1Ã—7 + 7Ã—1 = 14 ops â†’ two 3Ã—3 = 18 ops
```

---

## **6. Inception-v4 / Inception-ResNet-v2**

> **Not in `torchvision` by default**, but available via **timm** or custom impl.

```bash
pip install timm
```

```python
import timm

# Inception-v4
model = timm.create_model('inception_v4', pretrained=True, num_classes=10)

# Inception-ResNet-v2
model = timm.create_model('inception_resnet_v2', pretrained=True, num_classes=10)
```

---

## **7. Performance Comparison**

| Model | Params | Top-5 Error | Speed |
|------|--------|-------------|-------|
| **Inception-v3** | 25M | **5.6%** | Fast |
| **Inception-v4** | 43M | **4.9%** | Medium |
| **ResNet-152** | 60M | 5.7% | Slow |
| **DenseNet-201** | 20M | 6.2% | Slow |

---

## **8. Inception Module Evolution**

```text
v1: 1Ã—1, 3Ã—3, 5Ã—5
v2: 5Ã—5 â†’ 3Ã—3Ã—2
v3: 5Ã—5 â†’ 1Ã—5+5Ã—1, 7Ã—7 â†’ 3Ã—3Ã—2
v4: + Residual connections
```

---

## **9. Parameter Efficiency (Chart.js)**

```chartjs
{
  "type": "bar",
  "data": {
    "labels": ["Inception-v1", "v3", "v4", "ResNet-50"],
    "datasets": [{
      "label": "Parameters (M)",
      "data": [6.8, 25, 43, 25.6],
      "backgroundColor": ["#36A2EB", "#FF6384", "#FFCE56", "#4BC0C0"]
    }]
  },
  "options": {
    "plugins": { "title": { "display": true, "text": "Inception Family" } }
  }
}
```

---

## **10. Pros & Cons**

| Model | Pros | Cons |
|------|------|------|
| **v3** | Efficient, fast, accurate | No residual |
| **v4** | Best accuracy | More params, complex |

---

## **11. Summary**

| Feature | Inception-v3 | Inception-v4 |
|--------|--------------|-------------|
| **Year** | 2015 | 2016 |
| **Key** | Factorization, BN | Residual + Inception |
| **Params** | 25M | 43M |
| **Top-5** | 5.6% | **4.9%** |
| **Use Case** | Mobile, speed | Max accuracy |

---

## **Run Code**

```bash
pip install torch torchvision timm
python inception_v3.py
```

> Expected: **~96%+ on CIFAR-10**

---

**Inception-v3 is the gold standard for efficient deep CNNs.**  
Used in:
- MobileNet inspiration
- Object detection (SSD)
- Medical imaging
- Transfer learning

---
