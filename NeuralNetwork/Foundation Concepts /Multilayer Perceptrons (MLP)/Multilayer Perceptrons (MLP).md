**Multilayer Perceptrons (MLPs)** are a class of feedforward artificial neural networks that extend the single-layer perceptron to overcome its limitations, particularly the inability to handle non-linearly separable data. MLPs consist of multiple layers of interconnected nodes (neurons) and are capable of modeling complex, non-linear relationships, making them a cornerstone of modern machine learning and deep learning.

### Key Components of MLPs
1. **Layers**:
   - **Input Layer**: Receives input features (e.g., $\( x_1, x_2, \ldots, x_n \)$).
   - **Hidden Layers**: One or more layers of neurons that process inputs through weighted connections and non-linear activation functions. The number of hidden layers and neurons per layer determines the modelâ€™s complexity.
   - **Output Layer**: Produces the final output (e.g., class probabilities for classification or continuous values for regression).
2. **Neurons**: Each neuron computes a weighted sum of its inputs, adds a bias, and applies a non-linear activation function.
3. **Weights and Biases**: Each connection between neurons has a weight $\( w_{ij} \)$ , and each neuron has a bias $\( b_i \)$, which are adjusted during training.
4. **Activation Functions**: Non-linear functions (e.g., sigmoid, ReLU, tanh) enable MLPs to model complex patterns. Common choices:
   - **Sigmoid**: $\( \sigma(z) = \frac{1}{1 + e^{-z}} \)$ (outputs 0 to 1).
   - **tanh**: $\( \tanh(z) = \frac{e^z - e^{-z}}{e^Z + e^{-z}} \)$ (outputs -1 to 1).
   - **ReLU**: $\( \text{ReLU}(z) = \max(0, z) \)$ (outputs 0 or positive values).
5. **Loss Function**: Measures the error between predicted and actual outputs (e.g., mean squared error for regression, cross-entropy loss for classification).
6. **Backpropagation**: The training algorithm that uses gradient descent to update weights and biases by propagating errors backward through the network.

### Mathematical Representation
For a neuron in layer $\( l \)$, the output $\( a_j^{(l)} \)$ is:
<img width="298" height="139" alt="image" src="https://github.com/user-attachments/assets/cb7ae23e-eaaf-4759-ada3-6993ad6ca92e" />

where $\( w_{ij}^{(l)} \)$ is the weight from neuron $\( i \)$ in layer $\( l-1 \)$ to neuron $\( j \)$ in layer $\( l \), \( b_j^{(l)} \)$ is the bias, \( a_i^{(l-1)} \)$ is the activation from the previous layer, and $\( f \)$ is the activation function.

### Training Process
1. **Forward Pass**: Compute the output by passing inputs through the layers.
2. **Compute Loss**: Compare the predicted output to the target using the loss function.
3. **Backward Pass (Backpropagation)**:
   - Calculate gradients of the loss with respect to weights and biases using the chain rule.
   - Update weights and biases using gradient descent:
     <img width="450" height="100" alt="image" src="https://github.com/user-attachments/assets/d1b29812-ba13-4981-9e3d-4f5fac616b06" />

     where $\( \eta \)$ is the learning rate.
4. **Repeat**: Iterate over the dataset (epochs) until convergence or a stopping criterion is met.

### Advantages of MLPs
- **Non-Linear Modeling**: Hidden layers with non-linear activations allow MLPs to learn complex, non-linear decision boundaries, overcoming the linear separability limitation of single-layer perceptrons.
- **Universal Approximation**: MLPs with at least one hidden layer and sufficient neurons can approximate any continuous function (given enough neurons and appropriate weights).
- **Flexibility**: Suitable for various tasks, including classification, regression, and function approximation.
- **Feature Learning**: Automatically learns relevant features from raw data, reducing the need for manual feature engineering.

### Limitations of MLPs
1. **Computational Complexity**:
   - Training MLPs, especially with many layers or neurons, is computationally expensive due to matrix operations and backpropagation.
   - Requires significant memory and processing power for large datasets or deep architectures.
2. **Overfitting**:
   - MLPs with many parameters can overfit, memorizing training data instead of generalizing. Regularization techniques (e.g., dropout, L2 regularization) are often needed.
3. **Vanishing/Exploding Gradients**:
   - In deep MLPs, gradients can become very small (vanishing) or very large (exploding), making training difficult. Modern activation functions like ReLU and optimization techniques (e.g., batch normalization) mitigate this.
4. **Hyperparameter Sensitivity**:
   - Performance depends on choices like the number of layers, neurons, learning rate, and activation functions, requiring careful tuning.
5. **Lack of Interpretability**:
   - MLPs are often seen as "black boxes," making it hard to interpret the learned weights or decision-making process.
6. **Scalability Issues**:
   - For very high-dimensional data (e.g., images, videos), MLPs may require many parameters, making them less efficient than specialized architectures like convolutional neural networks (CNNs).

### Example: Solving XOR
The XOR problem, which a single-layer perceptron cannot solve due to its non-linear separability, can be addressed with an MLP:
- **Architecture**: 2 input neurons, 2 hidden neurons (with tanh activation), 1 output neuron (with sigmoid activation).
- **Inputs**: $\( (x_1, x_2) \in \{(0,0), (0,1), (1,0), (1,1)\} \)$.
- **Targets**: $\( 0, 1, 1, 0 \)$.
- **Training**: Backpropagation adjusts weights to form a non-linear decision boundary that correctly classifies all inputs.
- **Result**: The hidden layer learns features that combine inputs non-linearly, allowing the MLP to separate the classes.

### Applications
- **Classification**: Image recognition, sentiment analysis, spam detection.
- **Regression**: Predicting house prices, stock prices.
- **Function Approximation**: Modeling complex relationships in scientific or engineering data.
- **Foundation for Deep Learning**: MLPs are the basis for more advanced architectures like CNNs and recurrent neural networks (RNNs).

### Comparison with Perceptron
- **Perceptron**: Single layer, linear decision boundary, limited to linearly separable data, uses a simple learning rule.
- **MLP**: Multiple layers, non-linear decision boundaries, handles complex data, uses backpropagation for training.

