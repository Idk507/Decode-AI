{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c48db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e0d4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-3. -1.  0.  1.  3.]\n",
      "--------------------\n",
      "Sigmoid: [0.04742587 0.26894142 0.5        0.73105858 0.95257413]\n",
      "Tanh: [-0.99505475 -0.76159416  0.          0.76159416  0.99505475]\n",
      "ReLU: [0. 0. 0. 1. 3.]\n",
      "Leaky ReLU (alpha=0.1): [-0.3 -0.1  0.   1.   3. ]\n",
      "PReLU (alpha=0.2): [-0.6 -0.2  0.   1.   3. ]\n",
      "ELU (alpha=1.0): [-0.95021293 -0.63212056  0.          1.          3.        ]\n",
      "SELU: [-1.67056873 -1.11133074  0.          1.05070099  3.15210296]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    Range: (0, 1)\n",
    "    Use Case: Binary classification (output layer)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Tanh (Hyperbolic Tangent) activation function.\n",
    "    Range: (-1, 1)\n",
    "    Use Case: Hidden layers\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit) activation function.\n",
    "    Range: [0, infinity)\n",
    "    Use Case: Hidden layers (default)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU activation function.\n",
    "    Range: (-infinity, infinity)\n",
    "    Use Case: Hidden layers (fix dying ReLU)\n",
    "    Args:\n",
    "        x: Input array.\n",
    "        alpha: Slope for negative inputs (default 0.01).\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, x, x * alpha)\n",
    "\n",
    "def prelu(x, alpha):\n",
    "    \"\"\"\n",
    "    PReLU (Parametric ReLU) activation function.\n",
    "    Range: (-infinity, infinity)\n",
    "    Use Case: Deep networks (e.g., ResNet)\n",
    "    Args:\n",
    "        x: Input array.\n",
    "        alpha: Learnable parameter (can be a single value or an array matching x's shape).\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, x, x * alpha)\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    ELU (Exponential Linear Unit) activation function.\n",
    "    Range: (-alpha, infinity)\n",
    "    Use Case: Hidden layers (robust convergence)\n",
    "    Args:\n",
    "        x: Input array.\n",
    "        alpha: Scaling factor for negative inputs (default 1.0).\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def selu(x, alpha=1.6732632423543772, scale=1.0507009873554805):\n",
    "    \"\"\"\n",
    "    SELU (Scaled Exponential Linear Unit) activation function.\n",
    "    Range: (-lambda * alpha, infinity)\n",
    "    Use Case: Self-normalizing networks\n",
    "    Args:\n",
    "        x: Input array.\n",
    "        alpha: SELU alpha parameter.\n",
    "        scale: SELU scaling parameter.\n",
    "    \"\"\"\n",
    "    return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    test_input = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])\n",
    "\n",
    "    print(\"Input:\", test_input)\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    print(\"Sigmoid:\", sigmoid(test_input))\n",
    "    print(\"Tanh:\", tanh(test_input))\n",
    "    print(\"ReLU:\", relu(test_input))\n",
    "    print(\"Leaky ReLU (alpha=0.1):\", leaky_relu(test_input, alpha=0.1))\n",
    "\n",
    "    # For PReLU, alpha would typically be a learnable parameter in a neural network\n",
    "    # Here, we'll just use a fixed value for demonstration.\n",
    "    print(\"PReLU (alpha=0.2):\", prelu(test_input, alpha=0.2))\n",
    "\n",
    "    print(\"ELU (alpha=1.0):\", elu(test_input))\n",
    "    print(\"SELU:\", selu(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259a51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
