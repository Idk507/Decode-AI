
---

# Softplus Activation Function

## Mathematical Definition

The **Softplus** activation function is a smooth approximation of **ReLU**, defined as:

$$
\text{Softplus}(x) = \log(1 + e^x)
$$

---

## Mathematical Properties

* **Range**: $(0, \infty)$, since $\log(1 + e^x) > 0$ for all $x$.

* **Derivative**:

$$
\frac{d}{dx} \text{Softplus}(x) = \sigma(x) = \frac{1}{1 + e^{-x}}
$$

Where:

* $\sigma(x)$ is the **sigmoid function**.

* **Behavior**:

  * For large positive $x$: $\text{Softplus}(x) \approx x$
  * For large negative $x$: $\text{Softplus}(x) \to 0$
  * At $x = 0$: $\text{Softplus}(0) = \log(2) \approx 0.693$

* **Smoothness**: Completely smooth and differentiable across its domain.

---

## Use Cases

* **Smooth Approximation of ReLU**:

  * Preferred when differentiability is essential.
  * Useful in certain **generative models**, or optimization problems requiring smooth gradients.

---

## Advantages

* **Smooth and Differentiable**: No sharp transitions like ReLU.
* **Simple Derivative**: Its derivative is the sigmoid function, making it computationally straightforward.
* **Positive Outputs**: Ensures non-negative activations, similar to ReLU.

---

## Problems

* **Computationally Expensive**: Requires logarithmic and exponential calculations.
* **Not Zero-Centered**: Outputs strictly positive values, which can bias gradients.
* **No Negative Outputs**: Limits representation capacity compared to functions like **Swish** or **Mish**.

---

## Python Implementation

```python
import numpy as np
import matplotlib.pyplot as plt

# Softplus function
def softplus(x):
    return np.log(1 + np.exp(x))

# Derivative of Softplus (Sigmoid function)
def softplus_derivative(x):
    return 1 / (1 + np.exp(-x))

# Generate input values
x = np.linspace(-10, 10, 100)
y = softplus(x)
y_deriv = softplus_derivative(x)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(x, y, label='Softplus Function', color='blue')
plt.plot(x, y_deriv, label='Softplus Derivative', color='red', linestyle='--')
plt.title('Softplus Activation Function and Its Derivative')
plt.xlabel('Input (x)')
plt.ylabel('Output')
plt.legend()
plt.grid(True)
plt.show()
```

---

## Explanation of Code

* **Softplus Function**: Calculates $\log(1 + e^x)$, providing a smooth, ReLU-like output.
* **Derivative**: Outputs the sigmoid of the input, enabling smooth gradients.
* **Visualization**:

  * Demonstrates Softplusâ€™s smooth approximation to ReLU.
  * Derivative curve resembles the sigmoid function.

---

